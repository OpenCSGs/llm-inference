{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c57e967",
   "metadata": {},
   "source": [
    "In this notebook, we demostrated how to run an inference and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e6402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ddc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e8793",
   "metadata": {},
   "source": [
    "### Start ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70033eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "        },\n",
    "        \"working_dir\": \"/Users/lipeng/workspaces/git-devops/llm-ray\"\n",
    "    },\n",
    "    num_cpus=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a676c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31a07b",
   "metadata": {},
   "source": [
    "### Start single inference with UI\n",
    "`llm_experimental` will start a LLM inference, besides that, it will create simple UI based on the downstream `task` in the model defination file(yaml), you can:\n",
    "- Open UI(will print at the end of logs) in browser\n",
    "- Adopt `DeploymentHandle` to invoke the function supplied by `llm_experimental`. i.e. `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc989e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from llmserve.backend.server.run import llm_experimental\n",
    "from ray import serve\n",
    "from ray.serve.handle import DeploymentHandle, DeploymentResponse\n",
    "\n",
    "app_suite = llm_experimental(\"../models/text-generation--gpt2.yaml\")\n",
    "serve_name = app_suite[1][\"name\"]\n",
    "app = app_suite[0]\n",
    "handle: DeploymentHandle = serve.run(app, name=serve_name, route_prefix=\"/\" + serve_name)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb83d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = handle.query.remote(\"Python is kind of\")\n",
    "print(ray.get(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb1cb7",
   "metadata": {},
   "source": [
    "Delete inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete(serve_name, _blocking = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35420a",
   "metadata": {},
   "source": [
    "### Start multiple inferences with restful exposed\n",
    "\n",
    "Deploy multiple inference one time, and use them with restful api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce409a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmserve.backend.server.run import llm_server\n",
    "\n",
    "app = llm_server([\"../models/text-generation--gpt2.yaml\", \"../models/text-generation--facebook--opt-125m.yaml\"])\n",
    "serve_name = \"multiple\"\n",
    "handle: DeploymentHandle = serve.run(app, name=serve_name, route_prefix=\"/\" + serve_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03644135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(f\"http://127.0.0.1:8000/multiple/models\")\n",
    "print(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt2\"   # pick up one model from upper cells' output for testing\n",
    "\n",
    "data = '{\"prompt\": \"I want to\"}'\n",
    "res = requests.post(f\"http://127.0.0.1:8000/multiple/\"+ model +\"/run/predict/\", data=data)\n",
    "print(res.json()[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489151bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"facebook/opt-125m\".replace(\"/\", \"--\")   # pick up one model from upper cells' output for testing\n",
    "\n",
    "data = '{\"prompt\": \"I want to\"}'\n",
    "res = requests.post(f\"http://127.0.0.1:8000/multiple/\"+ model +\"/run/predict/\", data=data)\n",
    "print(res.json()[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb2fd8",
   "metadata": {},
   "source": [
    "Delete inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.delete(serve_name, _blocking = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm-ray",
   "language": "python",
   "name": ".llm-ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
